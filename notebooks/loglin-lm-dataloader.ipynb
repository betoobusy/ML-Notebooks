{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Language Model with Data Loader\n",
    "\n",
    "Status of Notebook: Work in Progress\n",
    "\n",
    "Difference from `loglin-lm.ipynb` is that we use a data loader to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入torch模块，用于深度学习相关操作\n",
    "import random  # 导入random模块，用于生成随机数\n",
    "import torch.nn as nn  # 导入torch.nn模块，用于定义神经网络模型\n",
    "import math  # 导入math模块，用于数学运算\n",
    "import time  # 导入time模块，用于计时\n",
    "import numpy as np  # 导入numpy模块，用于数值计算和数组操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download the datasets\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/test.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/train.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"读取数据文件并返回数据列表\"\"\"\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:  # 打开文件\n",
    "        for line in f:  # 逐行读取文件内容\n",
    "            line = line.strip().split(\" \")  # 去掉首尾空格并按空格分割成列表\n",
    "            data.append(line)  # 将每行数据加入到数据列表中\n",
    "    return data\n",
    "\n",
    "train_data = read_data('data/ptb/train.txt')  # 读取训练数据文件，返回训练数据列表\n",
    "val_data = read_data('data/ptb/valid.txt')  # 读取验证数据文件，返回验证数据列表\n",
    "\n",
    "word_to_index = {}  # 创建空字典，用于存储单词到索引的映射关系\n",
    "index_to_word = {}  # 创建空字典，用于存储索引到单词的映射关系\n",
    "\n",
    "word_to_index[\"<s>\"] = len(word_to_index)  # 将特殊符号\"<s>\"添加到字典中，并给它分配一个索引\n",
    "index_to_word[len(word_to_index)-1] = \"<s>\"  # 将索引与\"<s>\"的映射关系存储到字典中\n",
    "\n",
    "word_to_index[\"<unk>\"] = len(word_to_index)  # 将特殊符号\"<unk>\"添加到字典中，并给它分配一个索引\n",
    "index_to_word[len(word_to_index)-1] = \"<unk>\"  # 将索引与\"<unk>\"的映射关系存储到字典中\n",
    "\n",
    "def create_dict(data, check_unk=False):\n",
    "    \"\"\"根据数据列表创建字典，记录词汇和索引的映射关系\"\"\"\n",
    "    for line in data:  # 遍历数据列表的每一行\n",
    "        for word in line:  # 遍历每一行中的每个单词\n",
    "            if check_unk == False:  # 如果不需要检查未知单词\n",
    "                if word not in word_to_index:  # 如果单词不在字典中\n",
    "                    word_to_index[word] = len(word_to_index)  # 将单词添加到字典中，并分配一个索引\n",
    "                    index_to_word[len(word_to_index)-1] = word  # 将索引和单词的映射关系存储到字典中\n",
    "            else:  # 如果需要检查未知单词\n",
    "                if word not in word_to_index:  # 如果单词不在字典中\n",
    "                    word_to_index[word] = word_to_index[\"<unk>\"]  # 将单词的索引设为未知单词的索引\n",
    "                    index_to_word[len(word_to_index)-1] = word  # 将索引和单词的映射关系存储到字典中\n",
    "\n",
    "create_dict(train_data)  # 根据训练数据创建字典\n",
    "create_dict(val_data, check_unk=True)  # 根据验证数据创建字典，并检查未知单词情况\n",
    "\n",
    "# create word and tag tensors from data\n",
    "def create_tensor(data):\n",
    "    \"\"\"\n",
    "    将数据列表转换为张量的生成器函数\n",
    "    参数:\n",
    "        - data: 数据列表\n",
    "    返回值:\n",
    "        - 生成器对象，每次迭代产生一个张量\n",
    "    \"\"\"\n",
    "    for line in data:  # 遍历数据列表的每一行\n",
    "        yield [word_to_index[word] for word in line]  # 生成一个张量，将每个单词根据字典映射为对应的索引值\n",
    "\n",
    "train_data = [*create_tensor(train_data)]  # 使用生成器函数将训练数据转换为张量列表\n",
    "val_data = [*create_tensor(val_data)]  # 使用生成器函数将验证数据转换为张量列表\n",
    "number_of_words = len(word_to_index)  # 计算词汇表的长度，即单词的总数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PTB(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        PTB数据集的构造函数\n",
    "        参数:\n",
    "            - data: 数据列表\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集的长度\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引获取数据项\n",
    "        参数:\n",
    "            - idx: 索引值\n",
    "        返回值:\n",
    "            - 数据项的张量表示\n",
    "        \"\"\"\n",
    "        return torch.as_tensor(self.data[idx])\n",
    "\n",
    "train_dataset = PTB(train_data)  # 创建训练数据集对象\n",
    "val_dataset = PTB(val_data)  # 创建验证数据集对象\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # 创建训练数据集的数据加载器\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)  # 创建验证数据集的数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation we are using batched training. There are a few differences from the original implementation found [here](https://github.com/neubig/nn4nlp-code/blob/master/02-lm/loglin-lm.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # 检查CUDA是否可用并设置设备\n",
    "N = 2  # N-gram的长度\n",
    "\n",
    "class LogLinear(nn.Module):\n",
    "    def __init__(self, number_of_words, ngram_length):\n",
    "        super(LogLinear, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(number_of_words, number_of_words) for _ in range(ngram_length)])\n",
    "        # 创建嵌入层的 ModuleList，长度为 ngram_length\n",
    "        # 每个嵌入层都是一个 nn.Embedding 对象，用于将索引映射为词向量\n",
    "        # number_of_words 是词汇表的大小，嵌入层的维度与之相同\n",
    "        # 每个嵌入层都初始化为相同的权重矩阵\n",
    "\n",
    "        self.bias = torch.zeros(number_of_words, requires_grad=True).type(torch.FloatTensor).to(device)\n",
    "        # 创建偏置参数bias，维度为 number_of_words\n",
    "        # requires_grad=True 表示需要计算梯度\n",
    "        # 偏置参数初始化为全零\n",
    "\n",
    "        for i in range(N):\n",
    "            nn.init.xavier_uniform_(self.embeddings[i].weight)\n",
    "            # 使用 Xavier 初始化方法初始化每个嵌入层的权重矩阵\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = torch.cat([lookup(x) for x, lookup in zip(x.T, self.embeddings)]).view(N, x.shape[0], -1)\n",
    "        # 将输入数据 x 依次传入每个嵌入层，并将结果拼接在一起\n",
    "        # 这里使用了列表推导式和 zip 函数，将 x.T（转置后的 x）和嵌入层一一对应\n",
    "        # 得到的 embs 是一个形状为 N x batch_size x embedding_size 的张量\n",
    "        # 其中 N 是 ngram_length，batch_size 是输入数据的 batch 大小，embedding_size 是嵌入向量的维度\n",
    "\n",
    "        embs = torch.sum(embs, dim=0)  # 将 ngram 形式的嵌入向量相加，得到 batch_size x embedding_size 的张量\n",
    "        scores = embs + self.bias\n",
    "        # 将嵌入向量与偏置参数相加，得到表示得分的张量\n",
    "        # scores 的形状为 batch_size x number_of_words\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogLinear(number_of_words, N)  # 创建LogLinear模型，输入参数为单词数量和N\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # 使用Adam优化器来优化模型参数，学习率为0.1\n",
    "criterion = torch.nn.CrossEntropyLoss()  # 使用交叉熵损失函数\n",
    "\n",
    "if torch.cuda.is_available():  # 如果CUDA可用，则将模型移动到CUDA设备上进行加速\n",
    "\n",
    "def calc_sent_loss(sent):\n",
    "    S = word_to_index[\"<s>\"]  # 特殊标记<s>对应的索引\n",
    "    hist = [S] * N  # 初始历史记录，全部为<S>\n",
    "    all_targets = []  # 存储所有目标单词的列表\n",
    "    all_histories = []  # 存储所有历史记录的列表\n",
    "    for next_word in sent + torch.Tensor([S]):  # 遍历输入的句子和结束标记<S>\n",
    "        all_histories.append(list(hist))  # 将当前历史记录添加到列表中\n",
    "        all_targets.append(next_word)  # 将当前目标单词添加到列表中\n",
    "        hist = hist[1:] + [next_word]  # 更新历史记录，将最旧的单词移除，添加当前单词\n",
    "\n",
    "    logits = model(torch.LongTensor(all_histories).to(device))  # 将历史记录转换为张量，并通过模型获取logits\n",
    "    loss = criterion(logits, torch.LongTensor(all_targets).to(device))  # 计算损失\n",
    "    return loss\n",
    "\n",
    "MAX_LEN = 100  # 生成句子的最大长度\n",
    "\n",
    "def generate_sent():\n",
    "    S = word_to_index[\"<s>\"]  # 特殊标记<s>对应的索引\n",
    "    hist = [S] * N  # 初始历史记录，全部为<S>\n",
    "    sent = []  # 存储生成的句子的列表\n",
    "    while True:\n",
    "        logits = model(torch.LongTensor([hist]).to(device))  # 输入当前的历史记录，通过模型获取logits\n",
    "        p = torch.nn.functional.softmax(logits)  # 对logits进行softmax激活，得到概率分布（1 x 单词数量）\n",
    "        next_word = p.multinomial(num_samples=1).item()  # 从概率分布中根据多项式分布采样一个单词作为下一个单词\n",
    "        if next_word == S or len(sent) == MAX_LEN:  # 如果采样到的单词为结束标记<S>或者句子已达到最大长度，则结束生成过程\n",
    "            break\n",
    "        sent.append(next_word)  # 将当前采样的单词添加到句子中\n",
    "        hist = hist[1:] + [next_word]  # 更新历史记录，将最旧的单词移除，添加当前单词\n",
    "    return sent  # 返回生成的句子列表想·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 0: train loss/word=9.0947, ppl=8907.6500\n",
      "iter 0: dev loss/word=9.7668, ppl=17444.9221, time=1.76s\n",
      "in this case of the trade deficit of the globe weeks columnist months <unk> from a <unk> character succeed reflects as an effort will teaching mr. chestman was essentially flat to deal with the board is this time the <unk> an international <unk> machines are n't being any at this time you were n't disclosed this week it to take over a company said it will introduce a new york <unk> that since friday 's sharp swings in the field sales were down on N at a <unk> company said it will invest in quarterly profit by the new securities\n",
      "on monday at N yen $ N million navy contract for advanced there were <unk> when he 's no decision has been done by the bush administration has of new hampshire preferred holders total package that includes <unk> <unk> is that the full of only N to rise N N months of sept. N N share of $ N down N N N to N N to N this year and sales increased nearly N million shares outstanding as of that japan is starting in france spain italy and turkey late 1960s commissioner worthy of a food <unk> rose to\n",
      "speaking to build a giant <unk> corp. new york stock exchange during the first nine months <unk> charges for example banks <unk> station and gas production at the hands of our crowd efforts have been trying to plot against him the chief received the payment problem of that big institutions were never going to be loyal to try to <unk> units in the federal reserve <unk> onto the field <unk> with any securities by the irs recently said it will introduce a new york <unk> that replaced become known as <unk> <unk> resources inc. <unk> between what 's own decision\n",
      "these funds will be a <unk> it a better business he <unk> the market after the N <unk> after an a computer company for the defense plan and will come from a <unk> gene was missing acting expired award clients ' portfolios are the close of N million navy contract for <unk> an analyst with <unk> by saturday morning hat in big trading houses analysts expected to seek to clean up all <unk> says he is <unk> the best thing you do n't even the clutter of gold for current delivery of $ N million of $ N a vehicle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advertisers and advertising rates for the s&p N issue of the issues <unk> pace with rival very small amounts to veto the constitution <unk> sen coordinator of the big three <unk> the las vegas 's increased <unk> activity is only one or for one thing is important as of as many as N million navy contract for the government is <unk> by mr. <unk> has <unk> business conditions and the earnings or N on the firm of that this is that mr. gorbachev 's economic activity and only half of the proposal to reduce interest rates in the <unk> he\n",
      "--finished 5000 sentences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1861/185239032.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0msent_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# CHANGE to all train_data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m         \u001B[0mmy_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalc_sent_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mtrain_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mmy_loss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_1861/2590246674.py\u001B[0m in \u001B[0;36mcalc_sent_loss\u001B[0;34m(sent)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mhist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhist\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mnext_word\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m     \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_histories\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_targets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for ITER in range(10):  # 迭代训练循环，共进行10次迭代（可以根据需要更改为100次）\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    train_words, train_loss = 0, 0.0  # 初始化训练单词数和训练损失为0\n",
    "    for sent_id, sent in enumerate(train_loader):  # 遍历训练数据加载器中的每个句子\n",
    "        my_loss = calc_sent_loss(sent[0])  # 计算当前句子的损失\n",
    "        train_loss += my_loss.item()  # 累加损失值\n",
    "        train_words += len(sent)  # 累加训练单词数\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        my_loss.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        if (sent_id+1) % 5000 == 0:  # 每处理5000个句子显示一次进度\n",
    "            print(\"--finished %r sentences\" % (sent_id+1))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words)))\n",
    "    # 输出当前迭代的训练损失和困惑度\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    dev_words, dev_loss = 0, 0.0  # 初始化验证单词数和验证损失为0\n",
    "    start = time.time()  # 记录当前时间\n",
    "    for sent_id, sent in enumerate(val_loader):  # 遍历验证数据加载器中的每个句子\n",
    "        my_loss = calc_sent_loss(sent[0])  # 计算当前句子的损失\n",
    "        dev_loss += my_loss.item()  # 累加损失值\n",
    "        dev_words += len(sent)  # 累加验证单词数\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
    "    # 输出当前迭代的验证损失、困惑度和耗时\n",
    "\n",
    "    for _ in range(5):  # 生成5个句子进行展示\n",
    "        sent = generate_sent()  # 生成句子\n",
    "        print(\" \".join([index_to_word[x] for x in sent]))  # 将索引转换为单词，并以空格分隔打印句子中的单词"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
